package models

type ChatCompletionResponse struct {
	ID                string   `json:"id"`                 // Unique identifier for the chat completion.
	Object            string   `json:"object"`             // Type of the object, typically "chat.completion".
	Created           int64    `json:"created"`            // Timestamp when the chat completion was created.
	Model             string   `json:"model"`              // The model used for generating the completion.
	Choices           []Choice `json:"choices"`            // List of completion choices generated by the model.
	Usage             Usage    `json:"usage"`              // Token usage statistics.
	SystemFingerprint string   `json:"system_fingerprint"` // Fingerprint of the system configuration.
}

type Choice struct {
	Index        int       `json:"index"`              // Index of the choice in the list of choices.
	Message      Message   `json:"message"`            // The message generated by the model.
	LogProbs     *LogProbs `json:"logprobs,omitempty"` // Log probabilities of the tokens, if available.
	FinishReason string    `json:"finish_reason"`      // Reason why the completion finished.
}

type Message struct {
	Role             string `json:"role"`                        // Role of the message sender (e.g., "user", "assistant").
	Content          string `json:"content"`                     // Content of the message.
	ReasoningContent string `json:"reasoning_content,omitempty"` // Optional reasoning content.
}

type LogProbs struct {
	Tokens        []string             `json:"tokens,omitempty"`         // List of tokens.
	TokenLogProbs []float64            `json:"token_logprobs,omitempty"` // Log probabilities of each token.
	TopLogProbs   []map[string]float64 `json:"top_logprobs,omitempty"`   // Top log probabilities for each token.
}

type Usage struct {
	PromptTokens          int `json:"prompt_tokens"`            // Number of tokens used in the prompt.
	CompletionTokens      int `json:"completion_tokens"`        // Number of tokens used in the completion.
	TotalTokens           int `json:"total_tokens"`             // Total number of tokens used.
	PromptCacheHitTokens  int `json:"prompt_cache_hit_tokens"`  // Number of tokens served from cache.
	PromptCacheMissTokens int `json:"prompt_cache_miss_tokens"` // Number of tokens not served from cache.
}

// Stream Chat Request
// Afew things are different including the messages. The and token usage
// Look closesly you ll notice the differences

type StreamChatCompletionResponse struct {
	ID                string          `json:"id"`                 // Unique identifier for the chat completion.
	Object            string          `json:"object"`             // Type of the object, typically "chat.completion".
	Created           int64           `json:"created"`            // Timestamp when the chat completion was created.
	Model             string          `json:"model"`              // The model used for generating the completion.
	Choices           []StreamChoices `json:"choices"`            // List of completion choices generated by the model.
	Usage             *StreamUsage    `json:"usage"`              // Token usage statistics.
	SystemFingerprint string          `json:"system_fingerprint"` // Fingerprint of the system configuration.
}

type StreamChoices struct {
	Index        int `json:"index"`
	Delta        StreamDelta
	FinishReason string `json:"finish_reason"`
}

type StreamDelta struct {
	Role    string `json:"role,omitempty"`
	Content string `json:"content"`
}

type StreamUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}
